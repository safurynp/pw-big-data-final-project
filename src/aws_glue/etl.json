{
	"dag": {},
	"jobConfig": {
		"command": "glueetl",
		"description": "",
		"role": "arn:aws:iam::363710412655:role/LabRole",
		"scriptName": "etl.py",
		"version": "4.0",
		"language": "python-3",
		"scriptLocation": "s3://aws-glue-assets-363710412655-us-east-1/scripts/",
		"temporaryDirectory": "s3://aws-glue-assets-363710412655-us-east-1/temporary/",
		"timeout": 2880,
		"maxConcurrentRuns": 1,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxRetries": 0,
		"metrics": true,
		"observabilityMetrics": true,
		"security": "none",
		"bookmark": "job-bookmark-disable",
		"logging": true,
		"spark": true,
		"sparkConfiguration": "standard",
		"sparkPath": "s3://aws-glue-assets-363710412655-us-east-1/sparkHistoryLogs/",
		"serverEncryption": false,
		"glueHiveMetastore": true,
		"etlAutoScaling": false,
		"etlAutoTuning": false,
		"jobParameters": [],
		"tags": [],
		"connectionsList": [],
		"jobMode": "DEVELOPER_MODE",
		"name": "etl",
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"script": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import lit, col, to_date, expr\n\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n\n\n# Build DataFrames from the raw data\ndyf = glueContext.create_dynamic_frame.from_catalog(database='raw-data-crawler-database', table_name='atp_players')\natp_players_df = dyf.toDF()\n\ndyf = glueContext.create_dynamic_frame.from_catalog(database='raw-data-crawler-database', table_name='wta_players')\nwta_players_df = dyf.toDF()\n\ndyf = glueContext.create_dynamic_frame.from_catalog(database='raw-data-crawler-database', table_name='atp_rankings')\natp_rankings_df = dyf.toDF()\n\ndyf = glueContext.create_dynamic_frame.from_catalog(database='raw-data-crawler-database', table_name='wta_rankings')\nwta_rankings_df = dyf.toDF()\n\ndyf = glueContext.create_dynamic_frame.from_catalog(database='raw-data-crawler-database', table_name='atp_matches')\natp_matches_df = dyf.toDF()\n\ndyf = glueContext.create_dynamic_frame.from_catalog(database='raw-data-crawler-database', table_name='wta_matches')\n# Resovle the 'draw_size' type conflict\ndyf = dyf.resolveChoice(specs = [('draw_size', 'cast:long')])\nwta_matches_df = dyf.toDF()\n\n\n# Resolve duplicate player IDs between ATP and WTA data\n# Get the maximum player_id from atp_players_df\nmax_atp_player_id = atp_players_df.agg({\"player_id\": \"max\"}).collect()[0][0]\n\n# Create a window function to generate new IDs starting from max_atp_player_id + 1\nw = Window.orderBy(F.monotonically_increasing_id())\n\n# Add a new column 'new_player_id' starting from (max_atp_player_id + 1) and rename/drop columns\nwta_players_df = wta_players_df.withColumn('new_player_id', F.row_number().over(w) + max_atp_player_id) \\\n                               .withColumnRenamed('player_id', 'old_player_id') \\\n                               .withColumnRenamed('new_player_id', 'player_id')\n\n# DataFrame wta_rankings: Change 'player' column to 'player_id'\nwta_rankings_df = wta_rankings_df.join(wta_players_df, on=wta_rankings_df.player == wta_players_df.old_player_id, how='left') \\\n                                 .select(wta_rankings_df['*'], wta_players_df['player_id']) \\\n                                 .drop('player')\n\n# DataFrame wta_matches: Change 'winner_id' and 'loser_id' columns to follow the new 'player_id'\nwta_matches_df = wta_matches_df.join(wta_players_df, on=wta_matches_df.winner_id == wta_players_df.old_player_id, how='left') \\\n                               .select(wta_matches_df['*'], wta_players_df['player_id']) \\\n                               .drop('winner_id') \\\n                               .withColumnRenamed('player_id', 'winner_id')\nwta_matches_df = wta_matches_df.join(wta_players_df, on=wta_matches_df.loser_id == wta_players_df.old_player_id, how='left') \\\n                               .select(wta_matches_df['*'], wta_players_df['player_id']) \\\n                               .drop('loser_id') \\\n                               .withColumnRenamed('player_id', 'loser_id')\n\n\n# Join ATP and WTA data\n# Merge ATP and WTA players data (drop 'old_player_id' from WTA data first)\nwta_players_df = wta_players_df.drop('old_player_id')\nplayers_df = atp_players_df.union(wta_players_df)\n\n# Merge ATP and WTA rankings data (rename 'player' column to 'player_id' and add 'tours' column to ATP data)\natp_rankings_df = atp_rankings_df.withColumnRenamed('player', 'player_id')\natp_rankings_df = atp_rankings_df.withColumn('tours', lit(None).cast('int'))\nrankings_df = atp_rankings_df.union(wta_rankings_df)\n\n# Merge ATP and WTA matches data\nmatches_df = atp_matches_df.union(wta_matches_df)\nplayers_df = players_df.withColumn('dob', to_date(col('dob'), 'yyyyMMdd'))\nrankings_df = rankings_df.withColumn('ranking_date', to_date(col('ranking_date'), 'yyyyMMdd'))\nmatches_df = matches_df.withColumn('tourney_date', to_date(col('tourney_date'), 'yyyyMMdd'))\nplayers_df.printSchema()\n\n\n# Create new columns - winner games and loser games (from 'score')\n# Split the 'score' string by empty space and save as an array\nmatches_df = matches_df.withColumn('split_score', F.split('score', ' '))\n\n# Extract the first character of each element in the array, cast to int, and sum\nmatches_df = matches_df.withColumn('winner_games', F.expr(\"aggregate(split_score, 0, (acc, x) -> acc + cast(substr(x, 1, 1) as int))\"))\n\n# Extract the last character of each element in the array, cast to int, and sum\nmatches_df = matches_df.withColumn('loser_games', F.expr(\"aggregate(split_score, 0, (acc, x) -> acc + cast(substr(x, -1, 1) as int))\"))\n\n# Drop the 'split_score' column\nmatches_df = matches_df.drop('split_score')\n\n\n# Write the DataFrames to S3 in a parquet format\nmatches_df.write.mode('overwrite').format(\"parquet\").save(\"s3://processed-tennis-stats-data/matches\")\nplayers_df.write.mode('overwrite').format(\"parquet\").save(\"s3://processed-tennis-stats-data/players\")\nrankings_df.write.mode('overwrite').format(\"parquet\").save(\"s3://processed-tennis-stats-data/rankings\")\njob.commit()"
}